Project: Testability Refactoring Analyzer

Overview

The Testability Refactoring Analyzer is a static analysis tool that evaluates how testable a codebase is and explains
why certain code is difficult to unit test. Rather than generating tests, the tool identifies structural inhibitors to
testability and produces actionable, rules-based refactoring suggestions.

The goal is to help engineers improve unit test quality, reduce test brittleness, and refactor code toward more 
deterministic, isolated, and observable designsâ€”without relying on AI or runtime instrumentation.

The tool is intentionally heuristic-driven and conservative. It does not attempt to infer business intent or 
correctness. Instead, it focuses on structural properties that are well-known to impact testability in safety-critical,
embedded, and systems-oriented software.

â¸»

Goals
	â€¢	Quantify testability using explainable, deterministic metrics
	â€¢	Identify specific testability inhibitors at the function and class level
	â€¢	Provide concrete, actionable refactoring guidance
	â€¢	Require no AI, learning models, or runtime execution
	â€¢	Be extensible to C++ static analysis in future versions

â¸»

Non-Goals (v1)
	â€¢	Automatically generating unit tests
	â€¢	Inferring business logic correctness
	â€¢	Detecting performance issues
	â€¢	Runtime profiling or dynamic tracing
	â€¢	Perfect accuracy (heuristics are expected and documented)

â¸»

Inputs
	â€¢	One or more Python source files
	â€¢	Optional configuration file (future)
	â€¢	No test files or coverage data required in v1

â¸»

Outputs
	â€¢	Human-readable text report
	â€¢	Machine-readable JSON report

Each report includes:
	â€¢	Overall testability score (0â€“100)
	â€¢	Per-function and per-class findings
	â€¢	Ranked list of testability inhibitors
	â€¢	Refactoring suggestions mapped to detected issues

â¸»

Scoring Philosophy

The testability score is a heuristic indicator, not an absolute truth.
	â€¢	Start at 100 points
	â€¢	Subtract points for each detected inhibitor
	â€¢	The score is explainable and reproducible
	â€¢	Scores are comparable across revisions of the same codebase

â¸»

Intended Audience
	â€¢	Engineers working on legacy or safety-critical code
	â€¢	Developers refactoring for improved test coverage
	â€¢	Teams enforcing testability standards in CI
	â€¢	Engineers interested in static analysis and tooling

â¸»

Future Direction

Later versions may:
	â€¢	Integrate coverage data
	â€¢	Support C++ via Clang LibTooling
	â€¢	Provide refactor impact comparisons
	â€¢	Offer CI gating based on testability regression

â¸»

v1 Testability Rules (Exact)

The following rules define version 1 of the analyzer.
No additional rules will be added until v2.

â¸»

Rule 1: External Dependency Count

Detection
	â€¢	Count distinct external interactions inside a function:
	â€¢	File system access
	â€¢	Environment variables
	â€¢	Network calls
	â€¢	OS-level calls
	â€¢	Module-level singletons

Penalty
	â€¢	âˆ’5 points per distinct dependency type

Rationale
More dependencies require more mocking and setup, reducing unit test clarity.

Suggestion

Introduce dependency injection or extract logic into a pure function.

â¸»

Rule 2: Direct File I/O in Logic

Detection
	â€¢	Calls to open, file reads/writes, or filesystem APIs inside non-I/O-specific functions

Penalty
	â€¢	âˆ’10 points per function

Rationale
File I/O tightly couples logic to environment state.

Suggestion

Separate I/O from business logic and inject file interfaces.

â¸»

Rule 3: Non-Deterministic Time Usage

Detection
	â€¢	Calls to:
	â€¢	time.time
	â€¢	datetime.now
	â€¢	datetime.utcnow

Penalty
	â€¢	âˆ’10 points per function

Rationale
Time-dependent logic makes tests brittle and non-repeatable.

Suggestion

Inject a clock dependency.

â¸»

Rule 4: Randomness Usage

Detection
	â€¢	Calls to:
	â€¢	random.*
	â€¢	uuid.uuid*

Penalty
	â€¢	âˆ’10 points per function

Rationale
Randomness prevents deterministic assertions.

Suggestion

Inject a random number generator or seedable interface.

â¸»

Rule 5: Global State Mutation

Detection
	â€¢	Assignments to:
	â€¢	Module-level variables
	â€¢	Global objects
	â€¢	Singletons

Penalty
	â€¢	âˆ’10 points per function

Rationale
Global state causes test order dependence and hidden coupling.

Suggestion

Pass state explicitly or encapsulate it in an instance.

â¸»

Rule 6: Mixed I/O and Logic

Detection
	â€¢	A function that:
	â€¢	Performs I/O
	â€¢	AND returns computed values

Penalty
	â€¢	âˆ’8 points per function

Rationale
Mixing effects and logic prevents isolated testing.

Suggestion

Extract pure computation into a separate function.

â¸»

Rule 7: Branch Explosion Risk

Detection
	â€¢	Count conditional branches:
	â€¢	if / elif
	â€¢	match
	â€¢	Ternary expressions

Penalty
	â€¢	âˆ’2 points per branch after 3 branches

Rationale
High branching increases required test cases exponentially.

Suggestion

Simplify control flow or extract decision logic.

â¸»

Rule 8: Exception-Driven Control Flow

Detection
	â€¢	Exceptions raised as part of normal logic paths

Penalty
	â€¢	âˆ’5 points per function

Rationale
Exception paths are often under-tested and obscure intent.

Suggestion

Replace with explicit return values where appropriate.

â¸»

Rule 9: Constructor Side Effects (Classes)

Detection
	â€¢	__init__ methods that perform:
	â€¢	I/O
	â€¢	Time access
	â€¢	Global mutations
	â€¢	Network calls

Penalty
	â€¢	âˆ’15 points per class

Rationale
Side effects during construction complicate object setup in tests.

Suggestion

Defer work to explicit initialization methods.

â¸»

Rule 10: Hidden Dependencies via Imports-in-Function

Detection
	â€¢	Imports used inside functions instead of module scope

Penalty
	â€¢	âˆ’5 points per function

Rationale
Hidden dependencies are harder to mock and reason about.

Suggestion

Move dependencies to parameters or module-level injection points.

â¸»

Rule 11: Excessive Parameter Count

Detection
	â€¢	Functions with more than 5 parameters

Penalty
	â€¢	âˆ’5 points per function

Rationale
High arity often signals missing abstractions and test setup complexity.

Suggestion

Introduce parameter objects or refactor responsibilities.

â¸»

Rule 12: Low Observability

Detection
	â€¢	Functions that:
	â€¢	Perform work
	â€¢	Have no return value
	â€¢	And do not emit structured outputs

Penalty
	â€¢	âˆ’5 points per function

Rationale
Untestable outcomes reduce assertion quality.

Suggestion

Return meaningful results or expose observable state.

â¸»

End of v1 Ruleset





80â€“100 â€” Healthy
	â€¢	Tests should be straightforward
	â€¢	Refactors are low-risk
	â€¢	No action needed

60â€“79 â€” Caution
	â€¢	Tests are possible but increasingly brittle
	â€¢	Refactoring before adding features is recommended

40â€“59 â€” High Friction
	â€¢	Writing new tests is expensive
	â€¢	Refactors likely require test rewrites
	â€¢	Strong candidate for structural refactor

Below 40 â€” Refactor First
	â€¢	Tests are fragile or avoided
	â€¢	Changes are risky
	â€¢	Structural refactor is justified before feature work




This is important:

Refactor decisions are file-level.
Causes are function-level.

Rule of thumb
	â€¢	One bad function â‰  refactor file
	â€¢	Many medium-bad functions = refactor file

So:
	â€¢	File score = weighted aggregate of function scores
	â€¢	Highlight top offenders inside the file

Example output:

File testability score: 52 (High Friction)

Top contributors:
- process_order(): 34
- load_config(): 41
- OrderManager.__init__(): constructor side effects

Concrete numeric thresholds (recommended)

Here are thresholds that are:
	â€¢	Defensible
	â€¢	Not arbitrary-feeling
	â€¢	Conservative

Function-level
Score
Meaning
Action
â‰¥75
Easy to test
No action
55â€“74
Testable with effort
Consider refactor
35â€“54
Hard to test
Refactor recommended
<35
Painful
Refactor strongly recommended


File-level
Score
Meaning
Action
â‰¥80
Healthy
No action
60â€“79
Caution
Refactor opportunistically
40â€“59
High friction
Refactor before feature work
<40
Refactor first
Structural changes justified



Thresholds for specific red flags

Some things should trip warnings regardless of total score.

These are structural smells, not metrics.

Always-Flag Conditions

These should appear prominently even if score is â€œokayâ€:
	â€¢	Constructor (__init__) performs I/O
	â€¢	Global state mutation
	â€¢	Non-determinism in core logic
	â€¢	File does networking + business logic
	â€¢	Exception-driven control flow in hot paths

These are:

â€œThis will bite you laterâ€ signals

Think of them as yellow tape, not a stop sign.


â€œRefactor triggerâ€ heuristics (very realistic)

In real teams, refactors usually happen when:
	â€¢	Youâ€™re about to add tests
	â€¢	Youâ€™re about to add features
	â€¢	You just got burned by a bug

So your tool can phrase guidance like:
This file is in the "High Friction" band.

Recommendation:
- Refactor before adding new behavior
- Extract pure functions to improve isolation
- Introduce dependency injection for time and filesystem access


Donâ€™t over-weight complexity early

This is a common mistake.

A function with:
	â€¢	6 branches
	â€¢	but pure
	â€¢	deterministic
	â€¢	no dependencies

â€¦is still testable.

So:
	â€¢	Complexity penalties should be lighter than:
	â€¢	I/O
	â€¢	Globals
	â€¢	Time
	â€¢	Randomness


How to justify these thresholds (important)

If anyone ever questions them, your answer is simple and strong:

â€œThese thresholds reflect increasing marginal cost of writing reliable unit tests, not code quality.â€

Thatâ€™s a defensible engineering stance.

â¸»

8. Recommended v1 behavior (summary)

For v1, Iâ€™d hardcode:
	â€¢	The band thresholds above
	â€¢	A small set of always-flag conditions
	â€¢	Conservative language in output

Avoid:
	â€¢	Auto-failing builds
	â€¢	Hard refactor mandates
	â€¢	Absolute claims

â¸»

9. If you want one concrete next step

Iâ€™d recommend this next:

ðŸ‘‰ Create 3 example files
	1.	Healthy file
	2.	High-friction file
	3.	Refactor-first file

Then write the expected analyzer output before writing more code.

That will lock your thresholds and tone.

If you want, I can help you draft those example files and the expected reports next.